Activation functions decide whether a neuron should be activated or note by calculating the weighted sum and further adding bias with it. They are differentiable operators to transform input signal to outputs, while most of them add non-linearity. 

Some common activation functions are 

[[ReLU]] Function
Provides a very simple nonlinear transformation. Given the element z, the function takes the max w.r.t 0

[[Sigmoid]] Function
Takes inputs in the domain of R, and returns values in the range of (0,1)

[[Tanh]] Function
Takes inputs in the domain of R, and returns valus in the range of (-1,1)